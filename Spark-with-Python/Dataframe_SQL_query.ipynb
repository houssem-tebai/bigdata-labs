{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe running SQL query\n",
    "### Dr. Tirthajyoti Sarkar, Fremont, CA 94536\n",
    "### Relational data with flexible, powerful analytics\n",
    "Relational data stores are easy to build and query. Users and developers often prefer writing easy-to-interpret, declarative queries in a human-like readable language such as SQL. However, as data starts increasing in volume and variety, the relational approach does not scale well enough for building Big Data applications and analytical systems. Following are some major challenges.\n",
    "\n",
    "* Dealing with different types and sources of data, which can be structured, semi-structured, and unstructured\n",
    "* Building ETL pipelines to and from various data sources, which may lead to developing a lot of specific custom code, thereby increasing technical debt over time\n",
    "* Having the capability to perform both traditional business intelligence (BI)-based analytics and advanced analytics (machine learning, statistical modeling, etc.), the latter of which is definitely challenging to perform in relational systems\n",
    "\n",
    "We have had success in the domain of Big Data analytics with Hadoop and the MapReduce paradigm. This was powerful, but often slow, and gave users a low-level, procedural programming interface that required people to write a lot of code for even very simple data transformations. However, once Spark was released, it really revolutionized the way Big Data analytics was done with a focus on in-memory computing, fault tolerance, high-level abstractions, and ease of use.\n",
    "\n",
    "From then, several frameworks and systems like Hive, Pig, and Shark (which evolved into Spark SQL) provided rich relational interfaces and declarative querying mechanisms to Big Data stores. The challenge remained that these tools were either relational or procedural-based, and we couldn't have the best of both worlds.\n",
    "\n",
    "![spark-1](https://opensource.com/sites/default/files/uploads/2_hadoop-vs-spark.png)\n",
    "\n",
    "However, in the real world, most data and analytical pipelines might involve a combination of relational and procedural code. Forcing users to choose either one ends up complicating things and increasing user efforts in developing, building, and maintaining different applications and systems. Apache Spark SQL builds on the previously mentioned SQL-on-Spark effort called Shark. Instead of forcing users to pick between a relational or a procedural API, Spark SQL tries to enable users to seamlessly intermix the two and perform data querying, retrieval, and analysis at scale on Big Data.\n",
    "\n",
    "### Spark SQL\n",
    "Spark SQL essentially tries to bridge the gap between the two models we mentioned previously—the relational and procedural models.\n",
    "\n",
    "Spark SQL provides a DataFrame API that can perform relational operations on both external data sources and Spark's built-in distributed collections—at scale!\n",
    "\n",
    "To support a wide variety of diverse data sources and algorithms in Big Data, Spark SQL introduces a novel extensible optimizer called Catalyst, which makes it easy to add data sources, optimization rules, and data types for advanced analytics such as machine learning.\n",
    "Essentially, Spark SQL leverages the power of Spark to perform distributed, robust, in-memory computations at massive scale on Big Data. \n",
    "\n",
    "Spark SQL provides state-of-the-art SQL performance and also maintains compatibility with all existing structures and components supported by Apache Hive (a popular Big Data warehouse framework) including data formats, user-defined functions (UDFs), and the metastore. Besides this, it also helps in ingesting a wide variety of data formats from Big Data sources and enterprise data warehouses like JSON, Hive, Parquet, and so on, and performing a combination of relational and procedural operations for more complex, advanced analytics.\n",
    "\n",
    "![Spark-2](https://cdn-images-1.medium.com/max/2000/1*OY41hGbe4IB9-hHLRPuCHQ.png)\n",
    "\n",
    "### Spark SQL with Dataframe API is fast\n",
    "Spark SQL has been shown to be extremely fast, even comparable to C++ based engines such as Impala.\n",
    "\n",
    "![spark_speed](https://opensource.com/sites/default/files/uploads/9_spark-dataframes-vs-rdds-and-sql.png)\n",
    "\n",
    "Following graph shows a nice benchmark result of DataFrames vs. RDDs in different languages, which gives an interesting perspective on how optimized DataFrames can be.\n",
    "\n",
    "![spark-speed-2](https://opensource.com/sites/default/files/uploads/10_comparing-spark-dataframes-and-rdds.png)\n",
    "\n",
    "Why is Spark SQL so fast and optimized? The reason is because of a new extensible optimizer, **Catalyst**, based on functional programming constructs in Scala.\n",
    "\n",
    "Catalyst's extensible design has two purposes.\n",
    "\n",
    "* Makes it easy to add new optimization techniques and features to Spark SQL, especially to tackle diverse problems around Big Data, semi-structured data, and advanced analytics\n",
    "* Ease of being able to extend the optimizer—for example, by adding data source-specific rules that can push filtering or aggregation into external storage systems or support for new data types\n",
    "\n",
    "Catalyst supports both rule-based and cost-based optimization. While extensible optimizers have been proposed in the past, they have typically required a complex domain-specific language to specify rules. Usually, this leads to having a significant learning curve and maintenance burden. In contrast, Catalyst uses standard features of the Scala programming language, such as pattern-matching, to let developers use the full programming language while still making rules easy to specify.\n",
    "\n",
    "![catalyst-2](https://cdn-images-1.medium.com/max/1500/1*81ZOMxCci-tM2b-HNUX6Ww.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful references for this Notebook\n",
    "* [PySpark in Jupyter Notebook — Working with Dataframe & JDBC Data Sources](https://medium.com/@thucnc/pyspark-in-jupyter-notebook-working-with-dataframe-jdbc-data-sources-6f3d39300bf6)\n",
    "* [PySpark - Working with JDBC Sqlite database](http://mitzen.blogspot.com/2017/06/pyspark-working-with-jdbc-sqlite.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a SparkSession and read the a stock price dataset CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext as sc\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark1 = SparkSession.builder.appName('SQL').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark1.read.csv('Data/appl_stock.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a temporary view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('stock')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now run a simple SQL query directly on this view. It returns a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Date: timestamp, Open: double, High: double, Low: double, Close: double, Volume: int, Adj Close: double]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = spark1.sql(\"SELECT * FROM stock LIMIT 5\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+------------------+------------------+---------+------------------+\n",
      "|               Date|      Open|      High|               Low|             Close|   Volume|         Adj Close|\n",
      "+-------------------+----------+----------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04 00:00:00|213.429998|214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05 00:00:00|214.599998|215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06 00:00:00|214.379993|    215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07 00:00:00|    211.75|212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08 00:00:00|210.299994|212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "+-------------------+----------+----------+------------------+------------------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run slightly more complex queries\n",
    "How many entries in the `Close` field are higher than 500?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|count(Close)|\n",
      "+------------+\n",
      "|         403|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_greater_500 = spark1.sql(\"SELECT COUNT(Close) FROM stock WHERE Close > 500\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the average `Open` values of all the entries where `Volume` is either greater than 120 million or less than 110 million?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|         avg(Open)|\n",
      "+------------------+\n",
      "|309.12406365290224|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_1 = spark1.sql(\"SELECT AVG(Open) FROM stock WHERE Volume > 120000000 OR Volume < 110000000\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read a file (and create dataframe) by directly running a `spark.sql` method on the file\n",
    "Notice the syntax of `csv.<path->filename.csv>` inside the SQL query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales = spark1.sql(\"SELECT * FROM csv.`Data/sales_info.csv`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|    _c0|    _c1|  _c2|\n",
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "|   GOOG|    Sam|  200|\n",
      "|   GOOG|Charlie|  120|\n",
      "|   GOOG|  Frank|  340|\n",
      "|   MSFT|   Tina|  600|\n",
      "|   MSFT|    Amy|  124|\n",
      "|   MSFT|Vanessa|  243|\n",
      "|     FB|   Carl|  870|\n",
      "|     FB|  Sarah|  350|\n",
      "|   APPL|   John|  250|\n",
      "|   APPL|  Linda|  130|\n",
      "|   APPL|   Mike|  750|\n",
      "|   APPL|  Chris|  350|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read tables from a local SQLite database file using `JDBC` connection\n",
    "For this read, we will use the famous chinook DB in SQLite tutorial. You can [download the file from here](http://www.sqlitetutorial.net/sqlite-sample-database/). Remember to unzip it in a suitable directory. You will need the path to this file later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First `cd` to the directory where all the PySpark jar files are kept. Then download the SQLite jar file from the [given URL](https://mvnrepository.com/artifact/org.xerial/sqlite-jdbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Binary output can mess up your terminal. Use \"--output -\" to tell \n",
      "Warning: curl to output it to your terminal anyway, or consider \"--output \n",
      "Warning: <FILE>\" to save to a file.\n"
     ]
    }
   ],
   "source": [
    "!cd /home/tirtha/Spark/spark-2.3.1-bin-hadoop2.7/jars/\n",
    "!curl https://repo1.maven.org/maven2/org/xerial/sqlite-jdbc/3.28.0/sqlite-jdbc-3.28.0.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define driver, path to local .db file, and append that path to `jdbc:sqlite` to construct the `url`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = \"org.sqlite.JDBC\"\n",
    "path = \"Data/chinook.db\"\n",
    "url = \"jdbc:sqlite:\" + path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define `tablename` to be read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tablename = \"albums\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_albums = spark1.read.format(\"jdbc\").option(\"url\", url).option(\"dbtable\", tablename).option(\"driver\", driver).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------+\n",
      "|AlbumId|               Title|ArtistId|\n",
      "+-------+--------------------+--------+\n",
      "|      1|For Those About T...|       1|\n",
      "|      2|   Balls to the Wall|       2|\n",
      "|      3|   Restless and Wild|       2|\n",
      "|      4|   Let There Be Rock|       1|\n",
      "|      5|            Big Ones|       3|\n",
      "|      6|  Jagged Little Pill|       4|\n",
      "|      7|            Facelift|       5|\n",
      "|      8|      Warner 25 Anos|       6|\n",
      "|      9|Plays Metallica B...|       7|\n",
      "|     10|          Audioslave|       8|\n",
      "|     11|        Out Of Exile|       8|\n",
      "|     12| BackBeat Soundtrack|       9|\n",
      "|     13|The Best Of Billy...|      10|\n",
      "|     14|Alcohol Fueled Br...|      11|\n",
      "|     15|Alcohol Fueled Br...|      11|\n",
      "|     16|       Black Sabbath|      12|\n",
      "|     17|Black Sabbath Vol...|      12|\n",
      "|     18|          Body Count|      13|\n",
      "|     19|    Chemical Wedding|      14|\n",
      "|     20|The Best Of Buddy...|      15|\n",
      "+-------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_albums.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- AlbumId: integer (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- ArtistId: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_albums.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Don't forget to create temp view to use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_albums.createTempView('albums')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read another table - `artists`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tablename = \"artists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artists = spark1.read.format(\"jdbc\").option(\"url\", url).option(\"dbtable\", tablename).option(\"driver\", driver).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|ArtistId|                Name|\n",
      "+--------+--------------------+\n",
      "|       1|               AC/DC|\n",
      "|       2|              Accept|\n",
      "|       3|           Aerosmith|\n",
      "|       4|   Alanis Morissette|\n",
      "|       5|     Alice In Chains|\n",
      "|       6|Antônio Carlos Jobim|\n",
      "|       7|        Apocalyptica|\n",
      "|       8|          Audioslave|\n",
      "|       9|            BackBeat|\n",
      "|      10|        Billy Cobham|\n",
      "|      11| Black Label Society|\n",
      "|      12|       Black Sabbath|\n",
      "|      13|          Body Count|\n",
      "|      14|     Bruce Dickinson|\n",
      "|      15|           Buddy Guy|\n",
      "|      16|      Caetano Veloso|\n",
      "|      17|       Chico Buarque|\n",
      "|      18|Chico Science & N...|\n",
      "|      19|        Cidade Negra|\n",
      "|      20|        Cláudio Zoli|\n",
      "+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_artists.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artists.createTempView('artists')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test if SQL query is working fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|ArtistId|     Name|\n",
      "+--------+---------+\n",
      "|       1|    AC/DC|\n",
      "|       2|   Accept|\n",
      "|       3|Aerosmith|\n",
      "|       9| BackBeat|\n",
      "|      15|Buddy Guy|\n",
      "|      26|  Azymuth|\n",
      "|      36|  O Rappa|\n",
      "|      37| Ed Motta|\n",
      "|      46|Jorge Ben|\n",
      "|      50|Metallica|\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark1.sql(\"SELECT * FROM artists WHERE length(Name)<10 LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join the `albums` and `artists` tables in a single dataframe using SQL query (and order by `ArtistId`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = spark1.sql(\"SELECT * FROM artists LEFT JOIN albums ON artists.ArtistId=albums.ArtistId order by artists.ArtistId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-------+--------------------+--------+\n",
      "|ArtistId|                Name|AlbumId|               Title|ArtistId|\n",
      "+--------+--------------------+-------+--------------------+--------+\n",
      "|       1|               AC/DC|      1|For Those About T...|       1|\n",
      "|       1|               AC/DC|      4|   Let There Be Rock|       1|\n",
      "|       2|              Accept|      2|   Balls to the Wall|       2|\n",
      "|       2|              Accept|      3|   Restless and Wild|       2|\n",
      "|       3|           Aerosmith|      5|            Big Ones|       3|\n",
      "|       4|   Alanis Morissette|      6|  Jagged Little Pill|       4|\n",
      "|       5|     Alice In Chains|      7|            Facelift|       5|\n",
      "|       6|Antônio Carlos Jobim|      8|      Warner 25 Anos|       6|\n",
      "|       6|Antônio Carlos Jobim|     34|Chill: Brazil (Di...|       6|\n",
      "|       7|        Apocalyptica|      9|Plays Metallica B...|       7|\n",
      "|       8|          Audioslave|    271|         Revelations|       8|\n",
      "|       8|          Audioslave|     11|        Out Of Exile|       8|\n",
      "|       8|          Audioslave|     10|          Audioslave|       8|\n",
      "|       9|            BackBeat|     12| BackBeat Soundtrack|       9|\n",
      "|      10|        Billy Cobham|     13|The Best Of Billy...|      10|\n",
      "|      11| Black Label Society|     14|Alcohol Fueled Br...|      11|\n",
      "|      11| Black Label Society|     15|Alcohol Fueled Br...|      11|\n",
      "|      12|       Black Sabbath|     17|Black Sabbath Vol...|      12|\n",
      "|      12|       Black Sabbath|     16|       Black Sabbath|      12|\n",
      "|      13|          Body Count|     18|          Body Count|      13|\n",
      "+--------+--------------------+-------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_combined.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's the difference between temporary and global SQL views? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A temporary view does not persist (shared) across multiple sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|ArtistId|                Name|\n",
      "+--------+--------------------+\n",
      "|       1|               AC/DC|\n",
      "|       2|              Accept|\n",
      "|       3|           Aerosmith|\n",
      "|       4|   Alanis Morissette|\n",
      "|       5|     Alice In Chains|\n",
      "|       6|Antônio Carlos Jobim|\n",
      "|       7|        Apocalyptica|\n",
      "|       8|          Audioslave|\n",
      "|       9|            BackBeat|\n",
      "|      10|        Billy Cobham|\n",
      "+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_artists.createOrReplaceTempView(\"temp_artists\")\n",
    "\n",
    "df_temp = spark1.sql(\"SELECT * FROM temp_artists LIMIT 10\")\n",
    "df_temp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A new session is created but the temp view `temp_artists` cannot be accessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark2 = SparkSession.builder.appName('SQL2').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We use `try...except` to catch the error and display a generic message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_temp = spark2.sql(\"SELECT * FROM temp_artists LIMIT 10\")\n",
    "except:\n",
    "    print(\"Error happened in this execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, a global view is created in this session\n",
    "Global temporary view is tied to a system preserved database `global_temp`. So the view name must be referenced as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tablename = \"artists\"\n",
    "df_artists = spark2.read.format(\"jdbc\").option(\"url\", url).option(\"dbtable\", tablename).option(\"driver\", driver).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|ArtistId|                Name|\n",
      "+--------+--------------------+\n",
      "|       1|               AC/DC|\n",
      "|       2|              Accept|\n",
      "|       3|           Aerosmith|\n",
      "|       4|   Alanis Morissette|\n",
      "|       5|     Alice In Chains|\n",
      "|       6|Antônio Carlos Jobim|\n",
      "|       7|        Apocalyptica|\n",
      "|       8|          Audioslave|\n",
      "|       9|            BackBeat|\n",
      "|      10|        Billy Cobham|\n",
      "+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_artists.createOrReplaceGlobalTempView(\"global_artists\")\n",
    "\n",
    "df_global = spark2.sql(\"SELECT * FROM global_temp.global_artists LIMIT 10\")\n",
    "df_global.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start a new session. The view `global_artists` can be accessed across the sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark3 = SparkSession.builder.appName('SQL3').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|ArtistId|                Name|\n",
      "+--------+--------------------+\n",
      "|       1|               AC/DC|\n",
      "|       2|              Accept|\n",
      "|       3|           Aerosmith|\n",
      "|       4|   Alanis Morissette|\n",
      "|       5|     Alice In Chains|\n",
      "|       6|Antônio Carlos Jobim|\n",
      "|       7|        Apocalyptica|\n",
      "|       8|          Audioslave|\n",
      "|       9|            BackBeat|\n",
      "|      10|        Billy Cobham|\n",
      "+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_global = spark3.sql(\"SELECT * FROM global_temp.global_artists LIMIT 10\")\n",
    "df_global.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
